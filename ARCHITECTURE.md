# Architecture & Design Decisions

## Overview

This document outlines the architectural decisions, trade-offs, and rationale for the Product Feed Processor implementation.

## Core Design Principles

### 1. Memory Efficiency
**Decision**: Use streaming XML parser instead of DOM parser

**Rationale**:
- The feed file is 48MB, loading it entirely into memory would be inefficient
- Streaming allows processing arbitrarily large files with constant memory footprint
- xml-stream library provides event-based parsing built on SAX

**Trade-offs**:
- More complex code than simple DOM parsing
- Cannot randomly access elements
- Must maintain state as we stream

### 2. Batch Size Management
**Decision**: Dynamic batch building with size checking before each addition

**Rationale**:
- Requirement: batches must be "strictly below 5MB"
- Cannot predict product sizes upfront (descriptions vary greatly)
- Calculate JSON byte size after each addition to ensure we don't exceed limit

**Implementation**:
```javascript
// Test batch with new product
const testBatch = [...currentBatch, product];
const testSize = calculateBatchSize(testBatch);

// If would exceed limit, send current batch first
if (testSize >= MAX_BATCH_SIZE - BUFFER_SIZE) {
  sendBatch();
}
```

**Trade-offs**:
- Creates temporary arrays for size checking (garbage collection overhead)
- More CPU intensive than fixed-size batches
- But ensures maximum batch utilization while staying under limit

### 3. Field Extraction
**Decision**: Support both namespaced (g:id) and non-namespaced fields

**Rationale**:
- Google Merchant feeds can use different formats (RSS vs Atom)
- RSS uses `<g:id>` namespace, Atom may use `<id>`
- Makes code more robust for different feed formats

**Implementation**:
```javascript
id: item['g:id'] || item.id || ''
```

## Performance Characteristics

### Time Complexity
- **XML Parsing**: O(n) where n = number of products
- **Batch Building**: O(m) where m = products per batch (size checking)
- **Overall**: O(n Ã— m) but m is bounded by batch size limit

### Space Complexity
- **Memory Usage**: O(1) for streaming + O(m) for current batch
- **No memory growth** as file size increases (streaming architecture)

### Real-World Performance
From test with 48MB feed (50,039 products):
- Processing time: ~9 minutes
- Memory usage: Stable (no leaks)
- Batches created: 4 (3 at ~5MB, 1 partial)
- Products per batch: ~14,000-17,000

## Production Considerations

### What's Implemented âœ…
- Streaming parser for memory efficiency
- Smart batching under 5MB limit
- Error handling for missing files
- Comprehensive test suite (22 tests)
- Both RSS and Atom feed support

### Staff-Level Enhancements Added ðŸš€
- Configuration management (config.js)
- Structured logging with metrics (logger.js)
- Code style enforcement (.eslintrc.js)
- Git configuration (.gitignore)
- Architecture documentation (this file)

### Additional Improvements for Production ðŸ”„

#### 1. Error Handling & Resilience
**Current**: Basic error handling
**Needed**:
- Retry logic with exponential backoff for external service failures
- Dead letter queue for failed batches
- Validation errors collection (don't fail entire process for one bad product)
- Circuit breaker pattern for external service

#### 2. Observability
**Current**: Console logging
**Needed**:
- Structured JSON logging to stdout (for log aggregation)
- Metrics export (Prometheus, CloudWatch, etc.)
- Distributed tracing (OpenTelemetry)
- Health check endpoint

#### 3. Graceful Shutdown
**Current**: Abrupt process termination
**Needed**:
```javascript
process.on('SIGTERM', async () => {
  logger.info('SIGTERM received, finishing current batch...');
  await processor.sendBatch(); // Send partial batch
  await cleanup();
  process.exit(0);
});
```

#### 4. Checkpointing & Resume
**Current**: Start from beginning on failure
**Needed**:
- Periodic checkpointing of progress
- Resume from last checkpoint on restart
- Idempotency tokens for external service

#### 5. Rate Limiting
**Current**: Sends batches as fast as possible
**Needed**:
- Configurable rate limit for external service
- Backpressure handling
- Batch queuing with max queue size

#### 6. Input Validation
**Current**: Minimal validation
**Needed**:
- XML schema validation
- File size limits (prevent DoS)
- Required field validation with detailed errors
- Sanitization for XSS/injection prevention

#### 7. Monitoring & Alerts
**Needed**:
- Processing time alerts (slower than expected)
- Memory usage alerts
- Error rate alerts
- Batch size anomaly detection

## Testing Strategy

### Current Coverage
- Unit tests: Core functions (extraction, batching, sizing)
- Integration tests: XML parsing with fixtures
- Edge cases: Unicode, missing fields, invalid data

### Additional Testing for Production
- Load testing with various file sizes
- Performance regression tests
- Chaos engineering (network failures, service downtime)
- Memory leak detection
- Concurrency testing (if parallelized)

## Scalability Considerations

### Current Limitations
- Single-threaded processing
- Sequential batch sending
- No horizontal scalability

### Scaling Options

#### 1. Parallel Processing
Split large feeds into chunks and process in parallel:
```
Feed â†’ Split â†’ Worker 1 â†’ Batch Service
              â†’ Worker 2 â†’ Batch Service
              â†’ Worker 3 â†’ Batch Service
```

#### 2. Queue-Based Architecture
```
Parser â†’ Queue â†’ Batch Builder â†’ Queue â†’ External Service
         (SQS)                    (SQS)
```

Benefits:
- Decouples parsing from sending
- Natural backpressure handling
- Easy horizontal scaling
- Automatic retry/DLQ

#### 3. Streaming Pipeline
Use stream processing framework (Kafka Streams, Flink):
```
XML Stream â†’ Parse â†’ Batch â†’ Send
     â†“         â†“       â†“      â†“
  Scalable  Scalable Scalable Scalable
```

## Security Considerations

### Current Implementation
- No authentication/authorization
- No input sanitization
- No secrets management

### Production Requirements
- API key management for external service
- TLS for external service calls
- Input validation and sanitization
- Principle of least privilege (file system access)
- Audit logging (who processed what, when)

## Trade-offs & Future Improvements

### Trade-off: Simplicity vs Robustness
**Current**: Simple, readable code
**Alternative**: More robust with complex error handling
**Decision**: Start simple, add complexity as needed

### Trade-off: Memory vs Speed
**Current**: Optimized for memory (streaming)
**Alternative**: Load into memory, process faster with parallel operations
**Decision**: Memory optimization critical for large files

### Trade-off: Batch Utilization vs Latency
**Current**: Maximize batch size (close to 5MB)
**Alternative**: Fixed batch sizes (faster but less efficient)
**Decision**: Maximize utilization per requirements

## Conclusion

This implementation provides a solid foundation that:
- Meets all functional requirements
- Handles the 48MB feed efficiently
- Is well-tested and documented
- Can be evolved toward production-grade with the enhancements outlined above

The architecture is intentionally simple and focused on the core requirements, with clear paths for enhancement when production concerns become relevant.
